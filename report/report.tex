\documentclass[12pt]{article}

%\include{definitions}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{verbatimbox}
\usepackage[thinc]{esdiff}
\usepackage{url}
\usepackage[square,numbers]{natbib}
%To edit easily
\newcommand{\rem}[1]{\textbf{\color{red}[[#1]]}}
\newcommand{\add}[1]{\textbf{\color{blue}#1}}
\newcommand{\com}[1]{\textbf{\color{OliveGreen}[[#1]]}}
%\usepackage{lineno}
%\linenumbers
\usepackage{verbatim}
\usepackage{indentfirst}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{physics}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{xcolor}
%\usepackage{fancyhdr}
\usepackage[]{hyperref}
%\hypersetup{
%	colorlinks=true,
%	linkcolor=black,
%	filecolor=black,      
%	urlcolor=black,
%	citecolor=black
%}
\hypersetup{
	colorlinks=true,
	linkcolor={black},
	citecolor={cyan!80!black},
	filecolor={red!60!green},      
	urlcolor={magenta!60!black},
}


\usepackage[export]{adjustbox}
%\usepackage{mathptmx}
\usepackage{fix-cm}    
\makeatletter
\newcommand\HUGE{\@setfontsize\Huge{35}{40}}
\makeatother    

\newcommand*{\bfrac}[2]{\genfrac{\lbrace}{\rbrace}{0pt}{}{#1}{#2}}
\title{Learnable Non Euclidean Background Geometry for Embeddings}
\renewcommand\Authfont{\scshape\small}
\renewcommand\Affilfont{\itshape\footnotesize}
\author[1]{Bhavya Bhatt \thanks{Bhavyabhatt17@gmail.com}}
\affil[1]{Indian Institute of Technology Mandi, Mandi 175005, India}


\date{\today}



\begin{document}
\maketitle
%\vspace*{-10mm}
%\small
%\texttt{E-mail: $^1$\url{Bhavyabhatt17@gmail.com}}
%\normalsize

\begin{abstract}
This report contains my work that I did in my summer research internship at Siemens Technology and Services Pvt. Ltd., Bengaluru during the period of June 2019-August 2019. These notes are compilation of the current formalism used in embedding learning for knowledge graph and some of my original ideas (heavily influenced with non-euclidean embeddings\footnote{see references}) which can further be extended for experimentation in code. First section contains small introduction of the mathematical background of the problem of knowledge graph embedding learning and then few more exotic ideas are discussed in the succesive sections. The embedding learning method in machine learning have always targeted to learn relations and entity representations which can be projected into spacially analogous patterns using dimensional reduction techniques like PCA or manifold learning\footnote{one famous example is Word2Vec representations in which projected low dimensional manifold shows us that similar context words are closer to each other and dissimilar words are far apart}(these spacial patterns totally depends on the supervised task which we are using to learn the embeddings from, in our context it would become clear in a moment).
\end{abstract}

\newpage

\tableofcontents
\newpage


\section{Introduction}
A knowledge graph represents a DAG structure to model facts (or relations) between entities within some particular domain. Let's take an example of relations in a family, here family represents informally a domain, entities are the members of the family and relations represent specific blood relations like $isfather()$, $ismother()$ etc. We will only model two argument relations so for example Let $s$ be son and $f$ be father so according to the above defination we have
\[
isFather(s, f)=1
\]
\[
isMother(s, f)=0
\]One observation worth noting is the asymmetry in the relation which states that if we exchange the arguments, the value of the relation no longer remains same (as son is not father of his father) so 
\[
isFather(f, s)=0
\]
. Now the problem is to find new fact or predict new links between the entities based on the given knowledge graph. For example if we have been given 
\[
isFather(s, f)=1
\]
\[
isWife(f, m)=1
\] then based on these facts our probabilistic model should be able to predict that 
\[
isMother(s, m)=1
\]. Typically in real life application this knowledge graph can be exponentially large and can contain millions of such facts. Now to model the problem we first need vectorized representations of the entities and relations so as to make further mathematical modelling easier. Now these entities and relations should be composed in such a way (using some mathematical operations) that they encapsulate all the knowledge that is represented by the particular triplet\footnote{the set of two entity and a relation is called a triplet}. Now people have used many such compostional operations to model these links and we will discuss few popular ones which claim to provide state-of-the-art results on testing dataset. Before that let us first mathematically formalize the problem which we do in the next section. 
\subsection{Mathematical Preliminary}
We have a subject entity $s$ and object entity $o$ such that $s, o \in \Lambda$ where $\Lambda$ is the set of all entities in the domain and similarly we have set of all relations $P$ for which there exist a $r_{p}$ such that $r_{p} \subseteq \Lambda \times \Lambda $ with a mapping function $\phi: \Lambda \times \Lambda \mapsto \{0, 1\}$ to a real value $1$ or $0$ depending on whether the fact expressed by the relation and entities (passed as arguments) is true or false respectively. We also denote number of entities in $\Lambda$ with $n_{e}$ and number of relations in $P$ with $n_{r}$. We can then formally write our probabilistic model for all triplets $x_{i} = \{p, s, o\}_{i} \in P \times \Lambda \times \Lambda$ and labels $y_{i} \in \{0, 2\}$ with the dataset $D=\{x_{i}, y_{i}\}_{i=1}^{m}$ as follows
\begin{equation}
P(Y_{s, o, p}=1 | \Theta) = \sigma(\eta_{s, o, p}) = \sigma(r_{p}^{\intercal}e_{s}\circ e_{o})
\end{equation} where $\Theta=\{e_{i}\}_{i=1}^{n_{e}}\cup \{r_{p}\}_{j=1}^{n_{r}}$ is set of all parameters and $r_{p}$, $e_{s}$, $e_{o}$ are embeddings of relation, subject entity and object entity respectively such that $r_{p}, e_{s}, e_{o} \in \mathbb{R}^{K}$ where $K$ is number of dimensions of the embedding space and $\sigma$ is the sigmoid function to scale values between $0$ to $1$. The operator $\circ$ is the composition operator which should capture the meaning of how the entities combine with the relation to produce the fact as true or false. This operator is an attempt to model the interaction between a pair of entities and a relation in its abstract form and thus is very crucial to make sense of our model. We now discuss different choices of these compositional operator and associated model, each one tries to model different type of interaction\footnote{example of a interaction is symmetric relations in which the value of the fact is unaffected with the entity exchange in the argument}. One possible loss function for this supervised learning task\footnote{this is the supervised learning task in our context which we were talking about in the abstract} is standard logistic loss with $L_{2}$ regularizer as follows
\begin{equation}
L(\Theta) = \sum_{i=1}^{m}\log(1+\exp{-y_{i}\sigma(\eta_{s, o, r})_{i}}) + \lambda\|\Theta\|^{2}
\end{equation}
\section{Different Models}
\subsection{Euclidean Tensor Product composition: Modelling symmetric relations}
This model assumes a very simple yet computationally expensive compositional operator $\otimes$ which is a tensor product between two vectors and so our above model becomes as follows 
\begin{equation}
P(Y_{s, o, p}=1 | \Theta) = \sigma(\eta_{s, o, p}) = \sigma(e_{r}^{\intercal}e_{s}\otimes e_{o})
\end{equation} where $r_{p} \in \mathbb{R}^{K^2}$ and $e_{s}, e_{o} \in \mathbb{R}^{K}$. This model assumes that all the components of the embedded representations (both relation as well as entity) contribute in the output. This model is computationally expensive and many implementations assume a diagonal matrix for $r_{p}$ and thus this can only model symmetric relations. To overcome this we need some non-linear operation between the entity embeddings to be able to model anti-symmetric relations and at the same time having less total parameters to obey complexity bounds. Hence we move to the next model which tries to achieve this non-linear composition through circular correlation\footnote{more on this can be found in the related reference}.
\subsection{Holographic Embeddings}
In this model the composition operator is a so called holographic\footnote{term taken from assosiative memory} circular correlation which is defined as follows
\begin{equation}
(a \star b)_{k} = \sum_{i=0}^{K-1}a_{i}b_{k+i\ mod\ K}
\end{equation} and our model looks as follows
\begin{equation}
P(Y_{s, o, p}=1 | \Theta) = \sigma(\eta_{s, o, p}) = \sigma(e_{r}^{\intercal}e_{s}\star e_{o})
\end{equation} where $r_{p}, e_{s}, e_{o} \in \mathbb{R}^{K}$.
\subsection{Complex Embeddings and Hermitian Composition: Modelling anti-symmetric relations}
One another way to model anti-symmetric relations to use hermitian product for complex embeddings. So in this model all embeddings are in complex space and the operator used is mean of element-wise product between these complex embeddings of relation and entities with a consideration of two representations for each entity one is $e_{o}=e_{i}$ when it acts as object and $e_{s}=\bar{e_{i}}$ when it acts as subject where $\bar{e}=(e^{\star})^{\intercal}$ represents dagger of the complex vector $e$. The model is as follows
\begin{equation}
\eta_{s, o, p} = Re(<r_{p}e_{s}\bar{e_{o}}>) = Re(\sum_{k=1}^{K}r_{pk}e_{sk}\bar{e_{ok}})
\end{equation} where $r_{p}, e_{s}, e_{o} \in \mathbb{C}^{K}$. It is apparent that because of the dagger operation the composition is no more symmetric and thus this type of complex embedding is useful in modelling asymmetric or anti-symmetric relations which was not possible with diagonal tensor product composition.
\subsection{Complex Embeddings with simple constraints}
These are simple constraints on loss functions in case of complex embeddings and details can be refered from the original paper by Ding et al-2018.
\section{Learnable Non-Euclidean Embeddings}
This section discusses some of the original ideas which the author seems to have after learning different approaches that exist in the literature and attempts to give a unified view on the non-euclidean embedding methods some of which seems to work for specific background geometry (for example Hyperbolic geometry\footnote{see the reference for more details on hyperbolic geometry}). Some assumptions for this section is that embeddings exist in tangent space of a metric manifold whose intrinsic geometry is completely determined by the metric tensor\footnote{connections on the manifold are metric compatible - but we can also experiment with some other connections which results in torsion terms in our expressions and explore what results it give on the benchmark dataset}. Now we will briefly discuss about the differential geometric perspective of the infinitesimal distance in any arbitary geometry as follows

\bibliographystyle{apsrev4-1}
\bibliography{GRW}
% \begin{thebibliography}{20}
%
%\bibitem{RShankar}R Shankar,"Principles of Quantum Mechanics"
%
%\bibitem{GRW_CSL} 
%A.~Bassi and G.~C.~Ghirardi,
%``Dynamical reduction models,''
%Phys.\ Rept.\  {\bf 379}, 257 (2003)
%doi:10.1016/S0370-1573(03)00103-0
%[quant-ph/0302164].
%%%CITATION = doi:10.1016/S0370-1573(03)00103-0;%%
%%181 citations counted in INSPIRE as of 25 Jul 2018
%
%
%\bibitem{Pearle} Philip Pearle, Jiri Soucek ,"Path integrals for the Continuous Spontaneous Localization Theory", Foundations of Physics Letters, Vol. 2, No. 3, (1989)
%
%\bibitem{dentoket}X.X. Yi, S.X. Yu, "Effective Hamiltonian Approach to the Master Equation", DOI: 10.1088/1464-4266/3/6/304, arXiv:quant-ph/0008090v1.
%
%\bibitem{Pimon_1987} Pimon Ajanapon, "Classical limit of the path‐integral formulation of quantum mechanics in terms of the density matrix", American Journal of Physics 55, 159 (1987), https://doi.org/10.1119/1.15236
%
%\bibitem{moyal} J. E. Moyal, "Quantum mechanics as a statistical theory", https://doi.org/10.1017/S0305004100000487
%
%\bibitem{klein} U. Klein, "What is the limit →0 of quantum theory?", http://dx.doi.org/10.1119/1.4751274
%\end{thebibliography}
%

\end{document}
